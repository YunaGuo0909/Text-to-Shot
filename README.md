# ðŸŽ¬ AI-Driven Storyboard Generation

**Extending Text-to-Shot Diffusion for Multi-Shot Visual Pre-production with Camera Motion Trajectories**

> NCCA Masterclass Project â€” Extending *"From Script to Shot: Joint Generation of Camera Pose and Dual-Human 3D Actions"* (SIGGRAPH 2026)

---

## Overview

This project extends a SIGGRAPH 2026 paper that generates **a single static shot** (camera pose + two-character 3D poses) from text, into a full **multi-shot storyboard pipeline** with **dynamic camera motion trajectories**.

Given a scene description like *"Two people meet at a cafe, shake hands and sit down"*, the system automatically:

1. **Decomposes** the scene into a sequence of cinematic shots (via LLM)
2. **Generates** 3D character poses + camera framing for each shot (via diffusion model)
3. **Creates camera motion trajectories** for each shot (dolly, pan, track, crane, orbit, etc.)
4. **Renders** the complete storyboard as annotated panels with motion path overlays

```
Scene Description â”€â”€â–¶ Shot Decomposer â”€â”€â–¶ Diffusion Generator â”€â”€â–¶ Trajectory Generator â”€â”€â–¶ Storyboard Renderer
     (text)             (LLM-based)        (per-shot 3D)         (keyframe â†’ spline)        (panels + paths)
```

---

## Quick Start

### Prerequisites

- Python â‰¥ 3.10
- [uv](https://docs.astral.sh/uv/) package manager

### Installation

```bash
# Clone the repository
git clone https://github.com/<your-username>/masterclass02.git
cd masterclass02

# Install dependencies with uv
uv sync

# Run demo (no trained model needed)
uv run python generate_storyboard.py --demo
```

### Output

The demo generates:
- `outputs/demo_storyboard.png` â€” 6-panel storyboard with camera motion annotations
- `outputs/demo_trajectory_dolly_in.png` â€” Detailed trajectory parameter curves

---

## Project Structure

```
masterclass02/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml              # Model, training & rendering configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/                   # Neural network modules
â”‚   â”‚   â”œâ”€â”€ diffusion.py          # Gaussian diffusion process (DDPM)
â”‚   â”‚   â”œâ”€â”€ denoiser.py           # Joint 3-branch denoiser network
â”‚   â”‚   â”œâ”€â”€ film.py               # FiLM conditioning layer
â”‚   â”‚   â””â”€â”€ interaction.py        # Character-character & camera-character interaction
â”‚   â”œâ”€â”€ pipeline/                 # Generation pipeline modules
â”‚   â”‚   â”œâ”€â”€ shot_decomposer.py    # LLM-based scene â†’ shot decomposition
â”‚   â”‚   â”œâ”€â”€ storyboard_generator.py  # Multi-shot generation with coherence
â”‚   â”‚   â”œâ”€â”€ camera_trajectory.py  # Camera motion trajectory generation â˜…
â”‚   â”‚   â””â”€â”€ storyboard_renderer.py   # Visual storyboard rendering
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ dataset.py            # Dataset loading for training
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ toric.py              # Toric camera parameterization utilities
â”‚       â””â”€â”€ smpl_utils.py         # SMPL body model & 6D rotation utilities
â”œâ”€â”€ train.py                      # Model training script
â”œâ”€â”€ generate_storyboard.py        # Storyboard generation entry point
â”œâ”€â”€ pyproject.toml                # Dependencies (uv)
â”œâ”€â”€ PROJECT_PLAN.md               # 10-week project timeline
â””â”€â”€ TECHNICAL_DESIGN.md           # Detailed technical design document
```

---

## Technical Pipeline

### Stage 1 â€” Shot Decomposition (`shot_decomposer.py`)

Uses an LLM (GPT-4 / local model) to break a scene description into a structured shot list. Each shot specifies:
- **Shot type**: close-up, medium-shot, wide-shot, over-the-shoulder, two-shot
- **Camera motion**: static, dolly-in, dolly-out, pan-left, pan-right, crane-up, crane-down, track, orbit
- Character actions for both persons A and B

### Stage 2 â€” Joint Character-Camera Generation (`diffusion.py` + `denoiser.py`)

A **Gaussian Diffusion Model** generates the 3D configuration for each shot:

**Data representation** â€” Each shot is a 306-dim vector `y = (x_A, x_B, x_C)`:
- `x_A âˆˆ R^150`: Character A pose (22 joints Ã— 6D rotation + placement vector)
- `x_B âˆˆ R^150`: Character B pose (same structure)
- `x_C âˆˆ R^6`: Camera state in **Toric space** (pA_x, pA_y, pB_x, pB_y, Î¸, Ï†)

**Network architecture** â€” `JointDenoiser` with three parallel branches:
- Branch A (Character A), Branch B (Character B), Branch C (Camera)
- Each branch: Linear â†’ [MLP + FiLM conditioning + residual] Ã— 4 â†’ Linear
- Three **pairwise interaction modules** exchange messages between entities:
  - `I_HH`: Character A â†” Character B (e.g., handshake coordination)
  - `I_AC`: Character A â†” Camera (e.g., framing follows action)
  - `I_BC`: Character B â†” Camera

**Conditioning** â€” The denoiser is conditioned on:
- Text embedding (CLIP, 512-dim)
- Diffusion timestep (sinusoidal, 128-dim)
- Shot type embedding (learnable, 64-dim) â˜… *extension*

### Stage 3 â€” Camera Trajectory Generation (`camera_trajectory.py`) â˜… New

Extends the static Toric camera state into a temporal motion trajectory:

1. **Motion profile**: Each motion type (dolly, pan, crane, etc.) defines delta changes to the 6 Toric parameters
2. **Keyframe generation**: K keyframes are generated with easing functions (ease-in-out, quadratic)
3. **Spline interpolation**: Cubic spline (CÂ² continuous) interpolation produces smooth T-frame trajectories
4. **Evaluation metrics**: Velocity, acceleration, jerk (smoothness) are computed per trajectory

### Stage 4 â€” Storyboard Rendering (`storyboard_renderer.py`)

Renders all shots into a visual storyboard grid:
- Stick figure characters (A = red, B = cyan) from SMPL joint positions
- Camera motion arrows and path overlays (yellow)
- Shot type labels, camera motion type badges, descriptive text

---

## Training

```bash
# Train the diffusion model
uv run python train.py --config configs/default.yaml --device cuda
```

**Training process:**

1. Load dataset of `(text, character_A_pose, character_B_pose, camera_state)` tuples
2. For each batch:
   - Encode text with CLIP â†’ `text_embed (B, 512)`
   - Sample random timestep `t ~ Uniform(0, T)`
   - Add noise: `y_t = âˆšá¾±_t Â· y_0 + âˆš(1-á¾±_t) Â· Îµ`
   - Predict clean sample: `Å·_0 = f_Î¸(y_t, t, text_embed, shot_type)`
   - Compute loss: `L = MSE(Å·_0, y_0)`
3. Backpropagate with AdamW optimizer + gradient clipping
4. Save checkpoints every N epochs

**Key hyperparameters** (see `configs/default.yaml`):

| Parameter | Value |
|-----------|-------|
| Diffusion timesteps | 1000 |
| Beta schedule | Cosine |
| Hidden dim | 512 |
| Transformer layers | 4 Ã— 3 branches |
| Batch size | 64 |
| Learning rate | 1e-4 |
| Epochs | 500 |

---

## Camera Motion Types

| Type | Description | Toric Parameter Change |
|------|-------------|----------------------|
| `static` | Fixed camera | No change |
| `dolly-in` | Push toward subjects | pA, pB spread outward |
| `dolly-out` | Pull away from subjects | pA, pB move inward |
| `pan-left` | Rotate camera left | Î¸ decreases |
| `pan-right` | Rotate camera right | Î¸ increases |
| `crane-up` | Raise camera | Ï† increases |
| `crane-down` | Lower camera | Ï† decreases |
| `track` | Follow action laterally | Î¸ + slight dolly |
| `orbit` | Circle around subjects | Large Î¸ change |

---

## Acknowledgments

- Base paper: *"From Script to Shot: Joint Generation of Camera Pose and Dual-Human 3D Actions"* (SIGGRAPH 2026)
- Toric camera space: Lino & Christie (2015), ACM TOG
- DDPM: Ho, Jain & Abbeel (2020)
- FiLM conditioning: Perez et al. (2018)
- 6D rotation: Zhou et al. (2019), CVPR
- DanceCamera3D: Wang et al. (2024), AAAI
